# 技術説明資料

## 概要

本プログラムは、2-gramと転置インデックスを用いて、入力された検索語に一致する住所を検索するシステムです。  

プログラムは以下の2つの主要な処理で構成されています:
1. **転置インデックスファイルの作成**
2. **検索処理**

フロントエンドはStreamlitを使用しており、転置インデックスの作成や検索機能をUI上で操作可能です。

---

## プログラムの設計

### 1. 転置インデックスファイル作成

転置インデックスは、2-gram（2文字単位）で分割したトークンをキーとし、そのトークンが出現する住所データのインデックス集合（`set`型）をバリューとする辞書データで、データをJSON形式で保存します。

転置インデックスのバリューである住所インデックスの情報は、重複、順序が不要であるため、リストではなくPythonのsetを指定しています。

（なお、JSON形式でファイルに変換する際にリストに変換する処理を行っています。）
```
inverted_index = 
    {
        "北海": [0, 1, 2, ...],
        "海道": [0, 1, 2, ...],
        "道札": [0, 1, 2, ...],
        ...
        "東十": [9088, 9089, 11293, ...],
        "十条": [2055, 529, 530, ...],
        ...
    }
```

#### 処理フロー
1. **データの読み込み**:
   - CSVファイルから住所情報を`pandas.DataFrame`形式に変換。
   - 検索対象のカラムを抽出し、スペースを取り除いた文字列リストとして格納。

2. **住所文字列の2-gramによるトークン化**:
   - 各住所文字列を2文字単位で分割。

3. **転置インデックスの構築**:
   - トークンをキー、トークンが出現する住所データのインデックス集合(set)をバリューとして辞書を生成。

4. **データの保存**:
   - 転置インデックスの辞書をJSON形式で保存。

---

### 2. 検索処理

入力された検索語を2-gramで分割し、各トークンに対して一致する住所インデックスの集合を転置インデックスを参照して取得し、結果を確認して整形したものを出力します。

#### 処理フロー
1. **検索ワードの2-gramによるトークン化**:
   - 入力された検索語を2文字単位で分割。

2. **転置インデックスの参照**:
   - 分割された各トークンに対応する住所インデックス集合(set)を取得。
   - トークンの個数に応じて得られる複数のインデックス集合の積集合を取得し、すべてのトークンが含まれる住所を絞り込む。

3. **検索語の一致確認**:
   - 絞り込まれたインデックスに対応する住所文字列を確認し、検索語全体が含まれているか判定。
   - 特に、同一文字の二文字以上の繰り返し（例: "いい", "いいい"）の違いを区別。

4. **結果の出力**:
   - 元の`DataFrame`から、該当インデックスに対応する住所を取得。
   - 整形して、一覧形式で出力。



## なぜこの設計にしたか
課題で指示されている要件を確認した際、おおまかに以下のような処理手順になると考え、最終的に大きな変更はなくこのような設計になりました。
   - 住所データを一件ずつ、一つの文字列にする
   - 住所文字列を二文字ずつ分割する（2-gram）
   - 分割された単語と出現位置を結び付ける（転置インデックスを作成）
   - 転置インデックスを参照できるように検索語もトークンに分割する
   - 対象となる住所インデックスを取得する



### JSON形式で保存する理由
   - 確認しやすい (pickleはバイナリデータにな  ってしまうので読めない)


### CSVファイルをアップロード形式にした理由
   - CSVファイルはリポジトリに含めてはいけないという指示があったため、プログラムに埋め込むよりは適宜指定する形にするほうが良いと考えました。
   - 他の住所データのCSVファイルでもテストをする可能性があると考えました。

### streamlitで実装した理由
   - ファイルのアップロードやUI上で完結するため、適していると考えました。
   - 過去に触ったことがあり、シンプルなものであればUI作成もそれほど労力がかからないため。


## 備考
ソースコードにもdocstringやコメントを書いておりますので、細かな処理についてはそちらを参考にしていただけますと幸いです。
